{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Upload data folder to AWS s3\n",
    "* Overview of Pagination\n",
    "* Review Marker and MaxKeys\n",
    "* Develop Pagination using Marker and MaxKeys\n",
    "* Overview of AWS s3 Paginator\n",
    "* Develop Pagination using Paginator\n",
    "* Exercise and Solution - Paginate AWS s3 Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Upload data folder to AWS s3\n",
    "\n",
    "Upload files using AWS CLI Command.\n",
    "\n",
    "```shell\n",
    "aws s3 cp data/ \\\n",
    "    s3://itvawsdata/ \\\n",
    "    --recursive \\\n",
    "    --profile itvdev1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overview of Pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'itvawsdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_objects = s3_client.list_objects(Bucket=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s3_objects['Contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_objects = s3_client.list_objects(\n",
    "    Bucket=bucket,\n",
    "    MaxKeys=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s3_objects['Contents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Review Marker and MaxKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_objects = s3_client.list_objects(\n",
    "    Bucket=bucket,\n",
    "    MaxKeys=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_objects['Contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_objects['Marker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_objects['MaxKeys']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Develop Pagination using Marker and MaxKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objects = []\n",
    "marker = ''\n",
    "\n",
    "while True:\n",
    "    print(marker)\n",
    "    s3_objects = s3_client.list_objects(\n",
    "        Bucket=bucket,\n",
    "        MaxKeys=10,\n",
    "        Marker=marker\n",
    "    )\n",
    "    if s3_objects.get('Contents') is None:\n",
    "        break\n",
    "    all_objects.extend(s3_objects['Contents'])\n",
    "    marker = all_objects[-1]['Key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objects[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overview of AWS s3 Paginator\n",
    "\n",
    "1. No need to specify Marker.\n",
    "2. No need to worry about number of keys or objects in s3.\n",
    "3. We can specify bucket and prefix to paginate using AWS s3 paginator.\n",
    "4. The code will be cleaner using AWS s3 Paginator rather than using Marker and MaxKeys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = s3_client.get_paginator('list_objects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Develop Pagination using Paginator\n",
    "\n",
    "1. Create response iterator.\n",
    "2. Iterate through response iterator to iterate through object details.\n",
    "\n",
    "Note: Clean up all the objects from s3://itvawsdata so that you can take care of exercise without any issues.\n",
    "\n",
    "```shell\n",
    "aws s3 rm s3://itvawsdata/ --recursive --profile itvdev1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.setdefault('AWS_PROFILE', 'itvdev1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = input('Enter a bucket name: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = s3_client.get_paginator('list_objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objects = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_iterator = paginator.paginate(\n",
    "    Bucket=bucket\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in response_iterator:\n",
    "    all_objects.extend([item['Key'] for item in response['Contents']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objects[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exercise - Paginate AWS s3 Objects\n",
    "\n",
    "1. Setup NYSE Data with one file per day in s3 (run the provided code as demonstrated).\n",
    "  * s3 location: `s3://itvawsdata/nyse_all/nyse_data/`\n",
    "2. Use s3 Paginator to get total number of files as well as total size in the form of a tuple.\n",
    "3. Read data for 2007 January 10th into Data Frame using Pandas - `s3://itvawsdata/nyse_all/nyse_data/NYSE_2007010.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'data/nyse_all/nyse_data/NYSE_2007.txt.gz', \n",
    "    compression='gzip',\n",
    "    header=None,\n",
    "    names=['ticker', 'trade_date', 'open', 'high', 'low', 'close', 'volume']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is normal code to upload all the data to s3 by date.\n",
    "# We will use the app with multiprocessing to speed up the upload process.\n",
    "# Go to apps folder and run the app by using \"python app.py\"\n",
    "# Make sure to set environment variables for\n",
    "# BUCKET_NAME and NYSE_DATA_DIR\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "# AWS Setup\n",
    "s3 = boto3.client('s3')\n",
    "bucket = 'itvawsdata'  # Replace with your bucket name\n",
    "\n",
    "# Assuming you have a list of files in the format 'NYSE_YYYY.txt.gz'\n",
    "files = os.listdir('data/nyse_all/nyse_data')\n",
    "files = [f for f in files if f.startswith('NYSE_') and f.endswith('.txt.gz')]\n",
    "\n",
    "for file in files:\n",
    "    # Read the gzipped CSV into a DataFrame\n",
    "    df = pd.read_csv(\n",
    "        os.path.join('data/nyse_all/nyse_data', file),\n",
    "        compression='gzip',\n",
    "        header=None,\n",
    "        names=['ticker', 'trade_date', 'open', 'high', 'low', 'close', 'volume']\n",
    "    )\n",
    "\n",
    "    # Assuming trade_date is in the format YYYY-MM-DD\n",
    "    # Extract unique dates in that file\n",
    "    unique_dates = df['trade_date'].unique()\n",
    "\n",
    "    # Save to separate files based on the unique date\n",
    "    for trade_date in unique_dates:\n",
    "        date_df = df[df['trade_date'] == trade_date]\n",
    "\n",
    "        # Convert the DataFrame to CSV content\n",
    "        csv_content = date_df.to_csv(index=False)\n",
    "\n",
    "        # Define the key (path) in the S3 bucket\n",
    "        key = f'nyse_all/nyse_data/{trade_date}.csv'\n",
    "\n",
    "        # Upload the CSV content to S3\n",
    "        s3.put_object(Body=csv_content, Bucket=bucket, Key=key, ContentType='text/csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solution - Paginate AWS s3 Objects\n",
    "\n",
    "1. Setup NYSE Data with one file per day in s3 (run the provided code as demonstrated).\n",
    "  * s3 location: `s3://itvawsdata/nyse_all/nyse_data/`\n",
    "2. Use s3 Paginator to get total number of files as well as total size in MB.\n",
    "3. Read data for 2007 January 10th into Data Frame using Pandas - `s3://itvawsdata/nyse_all/nyse_data/NYSE_2007010.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = s3_client.get_paginator('list_objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_objects = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_iterator = paginator.paginate(\n",
    "    Bucket=bucket,\n",
    "    Prefix='nyse_all/nyse_data/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in response_iterator:\n",
    "    if response.get('Contents') is None:\n",
    "        break\n",
    "    all_objects.extend(response['Contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_count = len(all_objects)\n",
    "total_size = sum([obj['Size'] for obj in all_objects])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size in mb\n",
    "total_size / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deaws-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
