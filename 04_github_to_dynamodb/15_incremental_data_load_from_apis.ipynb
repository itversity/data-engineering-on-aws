{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Data Load from APIs\n",
    "\n",
    "Let us understand how to load the data incrementally from APIs into target database table in Dynamodb.\n",
    "* `GET /repositories` takes since to list public repositories beyond the id passed.\n",
    "* The repositories listed are pre sorted by id and hence we can keep track of last id and run the code.\n",
    "* Keep in mind that we can get up to approximately 4950 repo details with created_at per hour due to the GitHub rate limit.\n",
    "* Once we reach the threshold we need to wait for one hour to get new data. For now we will run the code until we reach the limit.\n",
    "* If we want to run using docker or AWS Lambda functions, we should plan for exit once the limit is reached and then restart the job approximately 1 hour after.\n",
    "* Either we can get up to 4900 using `GET /repositories` and then invoke `GET /repos/{owner}/{repo}` for those 4900 or we can process 100 at a time.\n",
    "* We will use the later approach and try to validate it for 300 repositories. \n",
    "  * Get 300 repositories using `GET /repositories`.\n",
    "  * Use `GET /repos/{owner}/{repo}` to get each of the repository details up to 50.\n",
    "  * Write all the 50 into the database.\n",
    "  * This approach will generate data continuously at regular intervals in Dynamodb table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_repos(token, since='333255899', batches=3):\n",
    "    repos = []\n",
    "    for i in range(batches):\n",
    "        print(f'Getting 100 repos from repo id {since} in {i + 1} iteration')\n",
    "        res = requests.get(\n",
    "            f'https://api.github.com/repositories?since={since}',\n",
    "            headers={'Authorization': f'token {token}'}\n",
    "        )\n",
    "        repos_listed = json.loads(res.content.decode('utf-8'))\n",
    "        repos += repos_listed\n",
    "        since = repos_listed[-1]['id']\n",
    "    return repos, since"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_details(owner, name, token):\n",
    "#     print(f'Getting repo details for {name}')\n",
    "    repo_details = json.loads(requests.get(\n",
    "        f'https://api.github.com/repos/{owner}/{name}',\n",
    "        headers={'Authorization': f'token {token}'}\n",
    "    ).content.decode('utf-8'))\n",
    "    return repo_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_repo_fields(repo_details):\n",
    "    repo_fields = {\n",
    "        'id': repo_details['id'],\n",
    "        'node_id': repo_details['node_id'],\n",
    "        'name': repo_details['name'],\n",
    "        'full_name': repo_details['full_name'],\n",
    "        'owner': {\n",
    "            'login': repo_details['owner']['login'],\n",
    "            'id': repo_details['owner']['id'],\n",
    "            'node_id': repo_details['owner']['node_id'],\n",
    "            'type': repo_details['owner']['type'],\n",
    "            'site_admin': repo_details['owner']['site_admin']\n",
    "        },\n",
    "        'html_url': repo_details['html_url'],\n",
    "        'description': repo_details['description'],\n",
    "        'fork': repo_details['fork'],\n",
    "        'created_at': repo_details['created_at']\n",
    "    }\n",
    "    return repo_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_repos(repos_details, table, batch_size=50):\n",
    "    with table.batch_writer() as batch:\n",
    "        repos_count = len(repos_details)\n",
    "        for i in range(0, repos_count, batch_size):\n",
    "            for repo in repos_details[i:i+batch_size]:\n",
    "                batch.put_item(Item=repo)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_load_repos(repos, token, table, batch_size=50):\n",
    "    repos_details = []\n",
    "    repos_size = len(repos)\n",
    "    for idx, repo in enumerate(repos):\n",
    "        if (idx + 1) % batch_size == 0:\n",
    "            print(f'Saving {idx + 1} out of {repos_size}')\n",
    "            load_repos(repos_details, table, batch_size)\n",
    "            repos_details = []\n",
    "        try:\n",
    "            owner = repo['owner']['login']\n",
    "            name = repo['name']\n",
    "            repo_details = get_repo_details(owner, name, token)\n",
    "            repo_fields = extract_repo_fields(repo_details)\n",
    "            repos_details.append(repo_fields)\n",
    "        except:\n",
    "            # We can log the exceptions into a log table to troubleshoot data quality issues\n",
    "            pass\n",
    "    load_repos(repos_details, table, batch_size)\n",
    "    # We can preserve the id of last repo in database so that we can restart\n",
    "    # after rate limit is reset\n",
    "    return repos_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting 100 repos from repo id 333255899 in 1 iteration\n",
      "Getting 100 repos from repo id 333256141 in 2 iteration\n",
      "Getting 100 repos from repo id 333256393 in 3 iteration\n"
     ]
    }
   ],
   "source": [
    "repos, since = list_repos('bd8a9c237cfd84a454a69ab4f68bc799d4d2e08f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodb = boto3.resource('dynamodb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghrepos_table = dynamodb.Table('ghrepos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 50 out of 300\n",
      "Saving 100 out of 300\n",
      "Saving 150 out of 300\n",
      "Saving 200 out of 300\n",
      "Saving 250 out of 300\n",
      "Saving 300 out of 300\n",
      "CPU times: user 5.26 s, sys: 296 ms, total: 5.56 s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "repos_details = get_and_load_repos(repos, 'bd8a9c237cfd84a454a69ab4f68bc799d4d2e08f', ghrepos_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deleting the old data so that we can reload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_repos(repos_details, ghrepos_table, batch_size=50):\n",
    "    with ghrepos_table.batch_writer() as batch:\n",
    "    \n",
    "        repos_count = len(repos_details)\n",
    "        for i in range(0, repos_count, batch_size):\n",
    "            print(f'Processing from {i} to {i+batch_size}')\n",
    "            for repo in repos_details[i:i+batch_size]:\n",
    "                key = {'id': repo['id']}\n",
    "                batch.delete_item(Key=key)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ghrepos_table.scan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rs['Items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs['Items'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 µs, sys: 0 ns, total: 19 µs\n",
      "Wall time: 21.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "delete_repos(rs['Items'], ghrepos_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
